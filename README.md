# visual-audio-cross-modal-retrieval

Code for Paper: DCLMA: Deep Correlation Learning with Multi-modal Attention for Visual-Audio Retrieval.
[[pdf]](https://dl.acm.org/doi/pdf/10.1145/3575658).
If you find this project useful, please cite the offical paper.
```
@article{zhang2022variational,
  title={Variational Autoencoder with CCA for Audio-Visual Cross-Modal Retrieval},
  author={Zhang, Jiwei and Yu, Yi and Tang, Suhua and Wu, Jianming and Li, Wei},
  journal={ACM Transactions on Multimedia Computing, Communications and Applications},
  year={2022},
  publisher={ACM New York, NY}
}
```

## Data
Download routines for the datasets are not provided in the repository. Please download and prepare the datasets yourself according to our paper:
- [VEGAS Dataset](https://drive.google.com/file/d/1EjRDkgiXzAR8thouBVJrj7hQg2WBUZ88/view?usp=share_link)
- [AVE Dataset](https://drive.google.com/file/d/1EjsbGoFZ2mCHNeVYmf45Kb4tNwTLV86o/view?usp=share_link)
- Original Dataset homepage:https://sites.google.com/view/audiovisualresearch

## Contact
If you have any questions, please email jiweizhang@nii.ac.jp
## Reference
Zhang, Jiwei, et al. "Variational Autoencoder with CCA for Audio-Visual Cross-Modal Retrieval." ACM Transactions on Multimedia Computing, Communications and Applications (2022).
